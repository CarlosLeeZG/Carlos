{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f09e6a-a8bd-4cb8-a1dd-7837364ce215",
   "metadata": {},
   "source": [
    "# 0. Import Libraries\n",
    "Before we start, we will import all the libraries needed for this Student Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db6595d-746a-4e21-84c7-f645547c2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning & Transformation\n",
    "import pandas as pd\n",
    "pd.set_option('display.max.columns', 99)\n",
    "import numpy as np\n",
    "\n",
    "# Data Understanding\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Data Visualisation\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning & Deep Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0bea8",
   "metadata": {},
   "source": [
    "# 1.üìùData Preparation\n",
    "---\n",
    "\n",
    "> **OVERALL GOAL:** \n",
    "> - Ingest the data and relabelling the outcome variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0507e2d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/lc_trainingset.csv\")\n",
    "test_df = pd.read_csv(\"data/lc_testset.csv\")\n",
    "\n",
    "train_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43ae67",
   "metadata": {},
   "source": [
    "## 1b. Determine train & test set dimension\n",
    "\n",
    "We understand the dimensionality of the data using <code>.shape</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db57e167",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\", train_df.shape)\n",
    "print(\"Test\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656da628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the missing column from test set\n",
    "set(train_df.columns.tolist()) - set(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b444a-f194-4036-b8af-837cbaba23a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Notice that the test dataset has 1 variable less than the training dataset and is identified to be the loan_status, also know as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56a9aa",
   "metadata": {},
   "source": [
    "## 1c. Check Out Loan Status\n",
    "\n",
    "We use <code>value_counts()</code> to see the number of instances of each unique status in the loan_status data column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6047610",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83fb426",
   "metadata": {},
   "source": [
    "We visualise the loan status for ease of interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895dc03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "While our descriptive analysis shows that the outcome variable only has 2 different outcomes, according to the business rules, it is possible that the column that we are trying to predict for has other different outcomes. As a good data scientist, we need to deal with this possibility so that if a data with a new outcome occurs the model will be able to deal with it accordingly. So how do we go about this?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f4ca4",
   "metadata": {},
   "source": [
    "## 1d. Relabelling the Loan Status\n",
    "\n",
    "From the above visualisation, we observed that around 10 type of loan status exist in this data set. We are only interested in 2 status i.e. <b>Defaulted</b> and <b>Not Defaulted</b>. Hence, we will need to add a new variable which will be binary (0s and 1s).\n",
    "\n",
    "- 0 means Not Defaulted\n",
    "- 1 means Defaulted\n",
    "\n",
    "All those loans, whose status is ‚ÄúFully Paid‚Äù, ‚ÄúCurrent‚Äù will be categorized as Not Defaulted and anything else will be categorized as Defaulted. To achieve this we will introduce new variable defaulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c83702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the function\n",
    "def change_loan_status(loan_status):\n",
    "    if loan_status in ['Fully Paid', 'Current']:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Next we apply the function\n",
    "train_df['loan_status'] = train_df['loan_status'].apply(change_loan_status)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1ca41",
   "metadata": {},
   "source": [
    "Once again, we apply the <code>value_counts()</code> to see the number of instances of each unique status in the loan_status data column. Now we can see that the loan status only has 2 category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['loan_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5b7b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "So in the real world, when your column to be predicted has more than 2 outcomes, and you wish you classify them into 2 different outcomes, the above is one of the approach you can take - using logic/domain-knowledge to categorise the classes together.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d71fe7-0b83-483c-8cdd-4397af4900a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform data cleaning. You may create as many cells as you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58948da4-628d-4def-b1ba-a57e70afcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.apply(lambda x: x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21427075",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy()\n",
    "train_df2['issue_d'] = pd.to_datetime(train_df2['issue_d'], format = \"%b-%Y\").dt.date\n",
    "train_df2['earliest_cr_line'] = pd.to_datetime(train_df2['earliest_cr_line'], format = \"%b-%Y\").dt.date\n",
    "train_df2[['issue_d', 'earliest_cr_line']]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737471d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df3 = train_df2.select_dtypes(include = ['object']).copy()\n",
    "train_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c3f00",
   "metadata": {},
   "source": [
    "# 2.üîçExploratory Data Analysis\n",
    "---\n",
    "\n",
    "> **OVERALL GOAL:** \n",
    "> - Get an understanding for which variables are important, view summary statistics, and visualize the data\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/08/how-to-perform-exploratory-data-analysis-a-guide-for-beginners/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc79014",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(train_df)\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3d32a",
   "metadata": {},
   "source": [
    "## 2a. Visualising Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672097ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train_df.columns.to_list()\n",
    "\n",
    "print('Missing Values in these columns:')\n",
    "for col in cols:\n",
    "    if len(train_df[train_df[col].isnull() == True]) != 0:\n",
    "        print(col, \"-\",  train_df2[col].isnull().sum())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed847723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62022e8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Doesnt seem possible for annual_inc, dti, revol_bal & revol_util to be 0. Hence, feature engineering needs to be implemented later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960d178",
   "metadata": {},
   "source": [
    "## 2b. Determine the distribution of loan status for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4feeda3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.term.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.term == i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993affb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.emp_length.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.emp_length == i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e68da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Distribution of loan status is similar across varying years of employment, hence to be removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2c63d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.sub_grade.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.sub_grade == i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4567082",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.home_ownership.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.home_ownership == i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a3253",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "For \"NONE\" and \"ANY\", they dont provide much context, hence to group them under \"OTHER\". Furthermore, there are only 2 record of \"ANY\" in the training set, which is not a good representation. Hence to avoid biasness, it is best to regroup under \"OTHER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.verification_status.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.verification_status \t == i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8420",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Verified and Source Verified have similar distribution of status loan and by nature both meant the same thing, hence to group them together in feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.purpose.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.purpose== i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cdfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.initial_list_status.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.initial_list_status== i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec8457",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Distribution of loan status is similar across varying different initial listing status of loan, hence to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_df.application_type.unique():\n",
    "    print(i)\n",
    "    print(f\"{train_df[train_df.application_type== i].loan_status.value_counts(normalize=True)}\")\n",
    "    print('==========================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212432e0",
   "metadata": {},
   "source": [
    "## 2c. Determine any collinearity between numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b164da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['loan_status'] = train_df['loan_status'].apply(str)\n",
    "\n",
    "corr = train_df.corr().round(3)\n",
    "fig = px.imshow(corr, color_continuous_scale = 'plasma', text_auto = True, aspect = 'auto')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49726a50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "From the heat map, we can deduce strong <b>positive</b> correlation (>=0.5) between different variables:\n",
    "- loan_amt ‚Üî installment\n",
    "- total_acc ‚Üî open_acc\n",
    "- pub_rec ‚Üî pub_rec_bankruptcies\n",
    "    \n",
    "There is an absence of strong <b>negative</b> correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe6d4ce",
   "metadata": {},
   "source": [
    "## 2d. Understand the Distribution of Loan Status with Correlated Variables\n",
    "### i) loan_amt ‚Üî installment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d6a21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Loan amount is the total sum borrowed whereas installment is the partial payment made towards repaying the loan amount, hence the high correlation and the possibility of duplicated column names <br>\n",
    "    \n",
    "Hence, let's study the distribution of these 2 variables with the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b36795",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(train_df, x='installment', y='loan_amnt', color='loan_status', template='simple_white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfe143",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "It is not easy to tell from the scatter plot as the dataset is unbalanced, having more records which have either fully paid their loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17db00a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['loan_status'] = train_df['loan_status'].apply(str)\n",
    "\n",
    "\n",
    "nbins_loan = round((np.max(train_df['loan_amnt']) - np.min(train_df['loan_amnt']))/1000)\n",
    "nbins_instal = round((np.max(train_df['installment']) - np.min(train_df['installment']))/50)\n",
    "\n",
    "trace1 = px.histogram(train_df, x = 'loan_amnt', color = 'loan_status',  height = 400, width = 600)\n",
    "trace2 = px.histogram(train_df, x = 'installment', color = 'loan_status', height = 400, width = 600)\n",
    "\n",
    "trace1.show()\n",
    "trace2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3debb996",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "It is observed that there is a clearer normal distribution for instalment where the peak is when instalment is around 300 - 400 dollars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49268164",
   "metadata": {},
   "source": [
    "### ii) Total_acc & Open_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5d5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['loan_status'] = train_df['loan_status'].apply(str)\n",
    "\n",
    "trace1 = px.histogram(train_df, x = 'total_acc', color = 'loan_status',  height = 300, width = 500)\n",
    "trace2 = px.histogram(train_df, x = 'open_acc', color = 'loan_status', height = 300, width = 500)\n",
    "\n",
    "trace1.show()\n",
    "trace2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c5e74",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Similar frequency distribution between the 2 variables, hence they can be considered as dependent variables.\n",
    "    \n",
    "However, both loan status peak around the same value for each variables, hence may not serve as good features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0e874",
   "metadata": {},
   "source": [
    "### iii) Pub_rec_bankruptcies and Pub_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da46ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['loan_status'] = train_df['loan_status'].apply(str)\n",
    "\n",
    "trace1 = px.histogram(train_df, x = 'pub_rec_bankruptcies', color = 'loan_status',  height = 500, width = 700)\n",
    "trace2 = px.histogram(train_df, x = 'pub_rec', color = 'loan_status', height = 500, width = 700)\n",
    "\n",
    "trace1.show()\n",
    "trace2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590df62",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e677ceda",
   "metadata": {},
   "source": [
    "# 3.üîÑ Feature Engineering\n",
    "---\n",
    "\n",
    "> **OVERALL GOAL:** \n",
    "> - Select relevant features and enhance them to improve the overall performance of the machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657808ac-d935-4f4a-a390-772806270d29",
   "metadata": {},
   "source": [
    "First we convert the text data into useful categorical numerical variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8652d",
   "metadata": {},
   "source": [
    "## 3a. Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572404b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Based on earlier EDA, we identify 6 columns consists of missing values. (See Section 2a)\n",
    "   \n",
    "<b><br><u> Dropping of columns with missing values </b></u>\n",
    "<br>Out of the 6, `emp_title` and `title` have high cardinality which will not be considered as useful features to be learned by model. As mentioned previously, normal distribution for `loan_status` across varying `emp_length` are similar. Hence these columns to be removed. (See Section 2b)\n",
    "    \n",
    "<b><br><u> Imputation of columns with missing values </b></u>\n",
    "<br>`revol_util`, `mort_ac`c and `pub_rec_bankruptcies` are numeric variables, but are right-skewed. Hence to impute using median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72762b85-b5c2-4922-9724-76db14414508",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df.copy()\n",
    "train_df2['revol_util'] = train_df2['revol_util'].fillna(train_df2['revol_util'].median())\n",
    "train_df2['mort_acc'] = train_df2['mort_acc'].fillna(train_df2['mort_acc'].median())\n",
    "train_df2['pub_rec_bankruptcies'] = train_df2['pub_rec_bankruptcies'].fillna(train_df2['pub_rec_bankruptcies'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d08d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = train_df2.columns.to_list()\n",
    "\n",
    "print('Missing Values in these columns:')\n",
    "for col in cols:\n",
    "    if len(train_df[train_df2[col].isnull() == True]) != 0:\n",
    "        print(col, \"-\",  train_df2[col].isnull().sum())\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed8849a",
   "metadata": {},
   "source": [
    "## 3b. Treating of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac1c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c540b57",
   "metadata": {},
   "source": [
    "### i) Label Encoding for Ordinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2['term'] = train_df2['term'].str.strip().str[:2]\n",
    "train_df2['term'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535af8d4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "sub_grade is the subset of grade, hence sub_grade to be kept and discard grade which may cause multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7288c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = train_df2.sort_values(['sub_grade'])\n",
    "train_df2['sub_grade'].unique()\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "train_df2['sub_grade'] = labelencoder.fit_transform(train_df2['sub_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1777630",
   "metadata": {},
   "source": [
    "### iii) Date Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2['Year_Issued'] = pd.to_datetime(train_df2['issue_d'], format = '%b-%Y').dt.year\n",
    "train_df2['Year_Issued'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18c761e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df2['Year_cr_line'] = pd.to_datetime(train_df2['earliest_cr_line'], format = '%b-%Y').dt.year\n",
    "train_df2['Year_cr_line'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d6066",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "There seems to be missing data for the earlier parts of the years for both `issue_d` and `earliest_cr_line`, hence to drop these columns for incomplete covereage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbad4e1",
   "metadata": {},
   "source": [
    "### iv) Converting address to postal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ff834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df2['postalcode'] = train_df2['address'].str.split(\" \").str[-1]\n",
    "train_df2[['address', 'postalcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e2efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df2['postalcode'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532aef55",
   "metadata": {},
   "source": [
    "### v) Dropping of Unnecessary Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c16ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_df = train_df2.drop(['id', 'grade', 'emp_title', 'emp_length', 'initial_list_status', 'address', 'title',\n",
    "                            'Year_Issued', 'earliest_cr_line','issue_d'], axis =1)\n",
    "pseudo_df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d383ee",
   "metadata": {},
   "source": [
    "### vi) One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea45550",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pseudo_df.select_dtypes('object').columns.tolist()\n",
    "dummies.remove('loan_status') # have to do on separate row, cant put concurrently abv. remove mutates the list in-place\n",
    "dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5567d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df3 = train_df2.copy()\n",
    "train_df3['verification_status'] = np.where(train_df3['verification_status'] == 'Source Verified', 'Verified', train_df3['verification_status'])\n",
    "train_df3['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace = True)\n",
    "\n",
    "train_df3['home_ownership'] = train_df3['home_ownership'].str.title()\n",
    "train_df3['application_type'] = train_df3['application_type'].str.title()\n",
    "\n",
    "for i in dummies:\n",
    "    train_df3[i] = train_df3[i].str.title()\n",
    "    print(train_df3[i].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a430f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4 = pd.get_dummies(train_df3, columns = dummies, drop_first = True)\n",
    "# train_df4.drop(dummies, axis  = 1, inplace = True)\n",
    "train_df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4b684",
   "metadata": {},
   "source": [
    "## 3c. Treating of Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e44af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4.select_dtypes(['int64','float64']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef03e00",
   "metadata": {},
   "source": [
    "### annual_inc\n",
    "As mentioned in Section 2a, not logical that borrower could borrow money with annual income = 0. Hence, to impute using median\n",
    "\n",
    "Additionally, the annual income is extremely right-skewed with a wide range, hence to execute log transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4[train_df4['annual_inc']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53afb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4['annual_inc'].replace(0, train_df4['annual_inc'].median(), inplace = True)\n",
    "if len(train_df4[train_df4['annual_inc']==0]) == 0:\n",
    "    print(\"Imputation Successful\")\n",
    "else:\n",
    "    raise Exception(\"Imputation Unsucessful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190abb25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df4['annual_inc_log'] = (train_df4['annual_inc']).transform(np.log) # no need to +1 as 0 has been imputed\n",
    "train_df4[['annual_inc_log', 'annual_inc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49d182",
   "metadata": {},
   "source": [
    "### total_acc & open_acc\n",
    "`total_acc` is highly correlated with `open_acc` as shown in the heatmap under Section 2c. Hence, one of the columns has to be removed to minimise error arising from multicollinearity. As `total_acc` distribution is more than than `open_acc`, `open_acc` will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d020dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4.drop_duplicates(['total_acc', 'open_acc'])[['total_acc', 'open_acc']].sort_values(['total_acc', 'open_acc']).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009df14a",
   "metadata": {},
   "source": [
    "### pub_rec_bankruptcies & pub_rec\n",
    "Similar to above,  `pub_rec_bankruptcies` and `pub_rec` are highly correlated and similar distribution as mentioned in section 2d, pub_rec will be selected as derogatory records can be an early tell-tale signs to determine whether borrower will default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4.drop_duplicates(['pub_rec_bankruptcies', 'pub_rec'])[['pub_rec', 'pub_rec_bankruptcies']].sort_values(['pub_rec', 'pub_rec_bankruptcies']).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db8dac",
   "metadata": {},
   "source": [
    "### loan_amt & installment\n",
    "To drop installment, for the same reason as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8efe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df4.drop_duplicates(['loan_amnt', 'installment'])[['loan_amnt', 'installment']].sort_values(['loan_amnt', 'installment']).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7fa609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df5 = train_df4.drop(['open_acc', 'annual_inc', 'pub_rec_bankruptcies', 'installment', 'id', 'grade', 'emp_title', \n",
    "                            'emp_length', 'initial_list_status', 'address', 'title',\n",
    "                           'Year_Issued', 'earliest_cr_line','issue_d'], axis = 1)\n",
    "train_df5.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271d569",
   "metadata": {},
   "source": [
    "### dti\n",
    "Similar to annual_inc, does not make sense to have 0 for dti. \n",
    "\n",
    "As the dti data is skewed with range from 0.01 (ignoring 0) to 9999 (which may be an error), these outliers need to be removed. To only include data in the 1st to 99th percentile based on the report generated in Section 2 where the common values make up 99.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df5['dti']==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ee6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_df5['dti']==9999).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a03f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_lim =train_df5['dti'].quantile(0.01)\n",
    "upper_lim =train_df5['dti'].quantile(0.99)\n",
    "\n",
    "\n",
    "train_df6 = train_df5[(train_df5['dti'] < upper_lim) & (train_df5['dti'] > lower_lim)]\n",
    "print(\"max:\", np.max(train_df6['dti']), \"\\nmin:\", np.min(train_df6['dti']))\n",
    "print('Number of Records Removed:', (len(train_df5)-len(train_df6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169cea4",
   "metadata": {},
   "source": [
    "### revol_bal\n",
    "Similar to annual_inc, does not make sense to have 0 for `revol_bal`. \n",
    "\n",
    "As the `revol_bal` data is skewed with range from 1 (ignoring 0) to 1298783, these outliers need to be removed. To only include data in the 1st to 99th percentile based on the report generated in Section 2 where the common values make up 99.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cf614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_lim =train_df6['revol_bal'].quantile(0.01)\n",
    "upper_lim =train_df6['revol_bal'].quantile(0.99)\n",
    "\n",
    "\n",
    "train_df7 = train_df6[(train_df6['revol_bal'] < upper_lim) & (train_df6['revol_bal'] > lower_lim)]\n",
    "print(\"max:\", np.max(train_df7['revol_bal']), \"\\nmin:\", np.min(train_df7['revol_bal']))\n",
    "print('Number of Records Removed:', (len(train_df6)-len(train_df7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d7709",
   "metadata": {},
   "source": [
    "### revol_util\n",
    "Similar to annual_inc, does not make sense to have 0 for `revol_util`. \n",
    "\n",
    "As the `revol_bal` data is skewed with range from 0.01 (ignoring 0) to 892.3, these outliers need to be removed. To only include data in the 1st to 99th percentile based on the report generated in Section 2 where the common values make up 97.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc59797",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_lim =train_df7['revol_util'].quantile(0.01)\n",
    "upper_lim =train_df7['revol_util'].quantile(0.99)\n",
    "\n",
    "\n",
    "train_df8 = train_df7[(train_df7['revol_util'] < upper_lim) & (train_df7['revol_util'] > lower_lim)]\n",
    "print(\"max:\", np.max(train_df8['revol_util']), \"\\nmin:\", np.min(train_df8['revol_util']))\n",
    "print('Number of Records Removed:', (len(train_df7)-len(train_df8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3249f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df8.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d81a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bde2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(train_df8.columns.tolist()) - set(test_df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a5d84-777e-4bf9-8659-b72ecfb6d5d8",
   "metadata": {},
   "source": [
    "# 4.ü§ñ Models Building and Comparing Model Perforamance\n",
    "---\n",
    "\n",
    "> **OVERALL GOAL:** \n",
    "> - Building a machine learning model that is capable of generating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf6aaf-d266-4397-8207-cdb6c280ad55",
   "metadata": {},
   "source": [
    "## 4a. Logistic Regression (baseline model without any feature engineering)\n",
    "\n",
    "Here we start off by creating a baseline model whhich you can use to compare against"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff4a1d9-a37e-46e0-9cb0-c30de3be8094",
   "metadata": {},
   "source": [
    "<b>Selecting the feature columns</b><br>\n",
    "We select the features we want to use in predicting our outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834951ae-1ee8-422b-abf8-189d4c9ff407",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selection = train_df5.columns.tolist()\n",
    "feat_selection.remove('loan_status')\n",
    "\n",
    "X = train_df5[feat_selection] # Select the features you want to use to predict the loan_status\n",
    "y = train_df5['loan_status'].astype(int)\n",
    "\n",
    "# X_kaggle = test_df[feature_cols] # answers to this are hidden. you can't use these 759338 rows for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a9e91-a29e-4c1a-80cc-abcafba8b8df",
   "metadata": {},
   "source": [
    "<b> Import the models of your choice</b><br>\n",
    "We select the features we want to use in predicting our outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354ffb4-be66-46a0-aa87-ddd445fe9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346df04b-e956-4009-b620-742c9afc3ef7",
   "metadata": {},
   "source": [
    "<b> Train-Test Split</b></br>\n",
    "We split the data to facilitate the evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaa5d1-4fd1-4983-b3fd-0a807f4731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96b7c5-945e-48b4-8636-66e4dc4364a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "You may opt for using k-fold cross validation as well.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f35c86-99ff-4eaa-b960-cddf1f937cec",
   "metadata": {},
   "source": [
    "<b> Evaluate your model (using only lc_trainingset.csv)</b><br>\n",
    "We generate a randomforest model by fitting the training data using <code>.fit()</code>, and thereafter generate predictions using <code>.predict()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ed2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583b9fc-8593-4dc0-abc4-95ca0f2f32ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = logreg_model.fit(X_train, y_train)\n",
    "y_pred_proba = logreg_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1a2fe-af2c-4847-b6f1-6b98503f365c",
   "metadata": {},
   "source": [
    "<b>We evaluate the model's AUC using <code>metrics.roc_auc_score()</code></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04119771-96b8-4d20-a41f-f01f65babfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('AUC:', metrics.roc_auc_score(y_test, y_pred_proba)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d64d2e",
   "metadata": {},
   "source": [
    "## 4b. Random Forest (using bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# bestEst = 0\n",
    "# bestDepth = 0\n",
    "# bestFeat = 0\n",
    "# bestAUC = 0\n",
    "# bestAcc = 0\n",
    "# print(\"n_estimator | max_depth | max_features | AUC | Accuracy\")\n",
    "\n",
    "# for est in [100, 200]:\n",
    "#     for depth in [2,4,8, None]:\n",
    "#         for maxfeat in ['sqrt', 'log2', None]:\n",
    "#             rf = RandomForestClassifier(n_estimators = est, max_depth = depth, max_features = maxfeat, random_state = 5)\n",
    "#             rf.fit(X_train, y_train)\n",
    "            \n",
    "#             y_pred_proba= rf.predict_proba(X_test)[:,1]\n",
    "#             y_predict = rf.predict(X_test)\n",
    "            \n",
    "#             auc = round(metrics.roc_auc_score(y_test, y_pred_proba), 4)\n",
    "#             accuracy = round(metrics.accuracy_score(y_test, y_predict), 4)\n",
    "            \n",
    "#             print(est,  \"|\", depth, \"|\", maxfeat, \"|\", auc, \"|\", accuracy)\n",
    "            \n",
    "#             if accuracy > bestAcc:\n",
    "#                 bestAUC = auc\n",
    "#                 bestEst = est\n",
    "#                 bestDepth = depth\n",
    "#                 bestFeat = maxfeat\n",
    "#                 bestAcc = accuracy\n",
    "#             else:\n",
    "#                 pass\n",
    "            \n",
    "# print(bestEst, \"|\", bestDepth, \"|\", bestFeat, \"|\", bestAUC, \"|\", bestAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a790abb",
   "metadata": {},
   "source": [
    "## 4c. XGBoost (using boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "bestAUC = 0\n",
    "bestdepth = 0\n",
    "bestlr = 0\n",
    "bestacc = 0\n",
    "print(\"Learning Rate | Max Depth | AUC | Accuracy\")\n",
    "for lr in range(1,6):\n",
    "    for depth in [2,4,8]:\n",
    "        xgboost = xgb.XGBClassifier(learning_rate=lr/10, max_depth = depth, booster = 'gbtree',\n",
    "                                    random_state=5)\n",
    "        xgboost.fit(X_train, y_train)\n",
    "        y_pred_proba= xgboost.predict_proba(X_test)[:,1]\n",
    "        y_predict = xgboost.predict(X_test)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "        auc_score = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "        print(lr/10, depth, round(auc_score,4), round(accuracy,4))\n",
    "        if accuracy > bestacc:\n",
    "            bestAUC = auc_score\n",
    "            bestlr = lr/10\n",
    "            bestdepth = depth\n",
    "            bestacc = accuracy\n",
    "        else:\n",
    "            pass\n",
    "print(bestlr, \"|\", bestdepth, \"|\", bestAUC, \"|\", bestacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce32d4d",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "feat_importances = pd.Series(xgboost.feature_importances_, index=X.columns).sort_values(ascending  = False)\n",
    "fig = px.bar(feat_importances)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22479d88",
   "metadata": {},
   "source": [
    "## 4d. Neural Network (using deep learning algorithm not taught in syllabus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf9ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# alpha = 0\n",
    "# learing_rate = 0\n",
    "# max_iteration = 0\n",
    "# tol = 0\n",
    "# n_iter = 0\n",
    "# bestAUC = 0\n",
    "# acc = 0\n",
    "\n",
    "# print('Alpha | Learning Rate | Max Iteration | Tolerance | AUC | Accuracy')\n",
    "# for a in [0.0001, 0.001]:\n",
    "#     for learning_rate in range(1,5):\n",
    "#         for max_iteration in [500, 1000]:\n",
    "#             for tol in [0.001, 0.005]:\n",
    "#                     perceptron = MLPClassifier(solver = 'adam', alpha = a,\n",
    "#                                                learning_rate_init = (learning_rate/10), max_iter = max_iteration,\n",
    "#                                                tol = tol,  random_state = 5)\n",
    "#                     perceptron.fit(X_train, y_train.ravel())\n",
    "#                     y_pred_proba= perceptron.predict_proba(X_test)[:,1]\n",
    "#                     y_predict = perceptron.predict(X_test)\n",
    "#                     auc_score = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "#                     accuracy = metrics.accuracy_score(y_test, y_predict)\n",
    "#                     print(a, learning_rate/10, max_iteration, tol, round(auc_score,4), round(accuracy, 4))\n",
    "#                     if auc_score > bestAUC:\n",
    "#                         bestAUC = round(auc_score,4)\n",
    "#                         alpha = a\n",
    "#                         learing_rate = learning_rate/10\n",
    "#                         max_iteration = max_iteration\n",
    "#                         tol = tol\n",
    "#                         acc = round(accuracy,4)\n",
    "#                     else:\n",
    "#                         pass\n",
    "# print(alpha, \"|\", learning_rate, \"|\", max_iteration, \"|\", tol, \"|\", bestAUC, \"|\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752689c-48dd-4176-aa09-443e50c93fa5",
   "metadata": {},
   "source": [
    "# 5.üìä Generate and Export Predictions from your Final Model\n",
    "---\n",
    "\n",
    "> **OVERALL GOAL:** \n",
    "> - Export your predictions and submit it to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b09891-3204-4fe2-a703-16e5026f4ed1",
   "metadata": {},
   "source": [
    "## 5a. Re-fit your final model on train.csv\n",
    "\n",
    "Say for instance you have determined the (based on accuracy)\n",
    "1. optimal basket of features, \n",
    "2. the most optimal model for this dataset, and \n",
    "3. the best parameters for the model you have chosen\n",
    "\n",
    "You should then retrain the chosen model with the optimal parameters, on the chosen basket of features on all the 316824 rows of data you have (instead of just on X_train, y_train). This is because you want to fully utilise your 316824 rows of data to maximise what your model can learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2180e06d",
   "metadata": {},
   "source": [
    "### Random Forest with tuned parameters for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7865c2a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Random Forest model is selected as it gives the highest accuracy among the 3 models tested under Section 4 with an AUC > 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eba637",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selection = train_df5.columns.tolist()\n",
    "feat_selection.remove('loan_status')\n",
    "feat_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_selection = train_df5.columns.tolist()\n",
    "feat_selection.remove('loan_status')\n",
    "\n",
    "X = train_df5[feat_selection] # Select the features you want to use to predict the loan_status\n",
    "y = train_df5['loan_status'].astype(int)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "xgboost = xgb.XGBClassifier(learning_rate=0.2, max_depth = 32, random_state=5)\n",
    "\n",
    "xgboost.fit(X,y)\n",
    "y_predict =xgboost.predict(X)\n",
    "y_pred_proba= xgboost.predict_proba(X)[:,1]\n",
    "accuracy = metrics.accuracy_score(y, y_predict)\n",
    "auc = metrics.roc_auc_score(y, y_pred_proba)\n",
    "print(accuracy, auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09cbb6b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5b. Generate predictions for the test_df\n",
    "\n",
    "Here, we will generate the predictions for the test_df. Make sure to apply whatever feature engineering technique you performed on train_df to test_df as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe88645-d671-4970-be93-fa8732ce54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the feature engineering you did to the train_df\n",
    "test_df = pd.read_csv('data/lc_testset.csv')\n",
    "test_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729da003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "test_df['mort_acc'] = test_df['mort_acc'].fillna(test_df['mort_acc'].median())\n",
    "\n",
    "# Label encoding for term and subgrades\n",
    "test_df['term'] = test_df['term'].str.strip().str[:2]\n",
    "\n",
    "test_df = test_df.sort_values(['grade'])\n",
    "test_df['sub_grade'].unique()\n",
    "test_df['sub_grade'] = LabelEncoder().fit_transform(test_df['sub_grade'])\n",
    "test_df.sort_index(inplace = True)\n",
    "\n",
    "# Date Extraction\n",
    "test_df['Year_Issued'] = pd.to_datetime(test_df['issue_d'], format = '%b-%Y').dt.year\n",
    "test_df['Year_cr_line'] = pd.to_datetime(test_df['earliest_cr_line'], format = '%b-%Y').dt.year\n",
    "\n",
    "# Address to Postal Code\n",
    "test_df['postalcode'] = test_df['address'].str.split(\" \").str[-1]\n",
    "\n",
    "# One Hot Encoding\n",
    "dummies = ['term', 'home_ownership', 'verification_status', 'purpose', 'application_type', 'postalcode']\n",
    "\n",
    "test_df['verification_status'] = np.where(test_df['verification_status'] == 'Source Verified', 'Verified', test_df['verification_status'])\n",
    "test_df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace = True)\n",
    "test_df['home_ownership'] = test_df['home_ownership'].str.title()\n",
    "test_df['application_type'] = test_df['application_type'].str.title()\n",
    "test_df['purpose'] = test_df['purpose'].str.title()\n",
    "\n",
    "test_df = pd.get_dummies(test_df, columns = dummies, drop_first = True)\n",
    "\n",
    "# Log Transform\n",
    "test_df['annual_inc'].replace(0, test_df['annual_inc'].median(), inplace = True)\n",
    "test_df['annual_inc_log'] = (test_df['annual_inc']).transform(np.log)\n",
    "\n",
    "# # Remove outlier\n",
    "# lower_lim =test_df['dti'].quantile(0.01)\n",
    "# upper_lim =test_df['dti'].quantile(0.99)\n",
    "# test_df = test_df[(test_df['dti'] < upper_lim) & (test_df['dti'] > lower_lim)]\n",
    "\n",
    "# lower_lim =test_df['revol_bal'].quantile(0.01)\n",
    "# upper_lim =test_df['revol_bal'].quantile(0.99)\n",
    "# test_df = test_df[(test_df['revol_bal'] < upper_lim) & (test_df['revol_bal'] > lower_lim)]\n",
    "\n",
    "# lower_lim =test_df['revol_util'].quantile(0.01)\n",
    "# upper_lim =test_df['revol_util'].quantile(0.99)\n",
    "# test_df = test_df[(test_df['revol_util'] < upper_lim) & (test_df['revol_util'] > lower_lim)]\n",
    "\n",
    "test_df = test_df.drop(['open_acc', 'pub_rec_bankruptcies', 'installment', 'id', 'grade', 'emp_title', 'annual_inc', \n",
    "                            'emp_length', 'initial_list_status', 'address', 'title',\n",
    "                           'Year_Issued', 'earliest_cr_line','issue_d'], axis = 1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95661012-0d7b-4888-9f4c-e355d71634ab",
   "metadata": {},
   "source": [
    "Here we use <code>.predict_proba</code> instead of <code>.predict()</code> because this is the format which the Kaggle platform requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3952dd4-38c3-4d7c-ab40-75f231fc2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_x = test_df #Extract the same features from the test set\n",
    "\n",
    "kaggle_x = scaler.fit_transform(kaggle_x)\n",
    "probabilities = xgboost.predict_proba(kaggle_x)\n",
    "\n",
    "kaggle_preds = probabilities[:,1]  # extract values from the rightmost column\n",
    "len(kaggle_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f998967",
   "metadata": {},
   "source": [
    "Make use of the <code>.to_csv()</code> function to output your predictions in the form of a csv, which will be the format you will be required to submit to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee19198",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = pd.DataFrame({\n",
    "    'id': list(range(len(kaggle_preds))),\n",
    "    'Predicted': kaggle_preds\n",
    "})\n",
    "output_dataframe.to_csv('data/my_predictions_xgboost_proba.csv', index=False)  \n",
    "\n",
    "# Check for the .csv in the same folder as your Jupyter Notebook\n",
    "# Try uploading this .csv to the Kaggle competition!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
